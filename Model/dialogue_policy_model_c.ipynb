{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/buwituze/pre-consultation-agent/blob/main/dialogue_policy_model_c.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "title",
      "metadata": {
        "id": "title"
      },
      "source": [
        "# üí¨ Model C ‚Äî Question-Flow / Dialogue Policy\n",
        "## Clinical Question Sequencing Model\n",
        "\n",
        "**Purpose:** Select the single most appropriate next question to ask a patient during a voice-based pre-consultation, based on what is already known and what is still missing.\n",
        "\n",
        "| | |\n",
        "|---|---|\n",
        "| **Input** | Patient state (from Model B) + conversation history |\n",
        "| **Output** | One next question ‚Äî plain text, voice-ready |\n",
        "| **Model** | Google Gemini AI (rule-guided prompting) |\n",
        "| **Mode** | Question selection only ‚Äî no diagnosis, no advice |\n",
        "\n",
        "### Pipeline Position\n",
        "```\n",
        "Model B Output                  Model C Output\n",
        "(Structured State)\n",
        "       ‚îÇ\n",
        "       ‚ñº\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ  Patient State      ‚îÇ\n",
        "‚îÇ  Missing Fields     ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ \"On a scale from 1 to 10,\n",
        "‚îÇ  Asked Questions    ‚îÇ         how severe is the pain?\"\n",
        "‚îÇ  Conversation Stage ‚îÇ\n",
        "‚îÇ  Safety Rules       ‚îÇ              ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚ñº\n",
        "                              ‚Üí Spoken to patient\n",
        "                              ‚Üí Answer fed back to Model B\n",
        "                              ‚Üí Loop continues\n",
        "```\n",
        "\n",
        "> ‚ö†Ô∏è **Hard Rules:** One question at a time. No diagnosis. No treatment advice. No medical interpretation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-1",
      "metadata": {
        "id": "section-1"
      },
      "source": [
        "---\n",
        "## üì¶ Section 1 ‚Äî Install & Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "install",
      "metadata": {
        "id": "install"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U google-generativeai\n",
        "\n",
        "import json\n",
        "import re\n",
        "from dataclasses import dataclass, field, asdict\n",
        "from typing import List, Optional\n",
        "from enum import Enum\n",
        "\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "\n",
        "print(\"‚úÖ Dependencies ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-2",
      "metadata": {
        "id": "section-2"
      },
      "source": [
        "---\n",
        "## üîë Section 2 ‚Äî API Key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "api-key",
      "metadata": {
        "id": "api-key"
      },
      "outputs": [],
      "source": [
        "# Add your key via: left sidebar ‚Üí üîë Secrets ‚Üí \"GEMINI_API_KEY\"\n",
        "try:\n",
        "    genai.configure(api_key=userdata.get(\"GEMINI_API_KEY\"))\n",
        "    print(\"‚úÖ API key loaded from Colab Secrets.\")\n",
        "except Exception:\n",
        "    genai.configure(api_key=\"YOUR_API_KEY_HERE\")  # fallback only\n",
        "    print(\"‚ö†Ô∏è  API key set manually. Use Colab Secrets for security.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-3",
      "metadata": {
        "id": "section-3"
      },
      "source": [
        "---\n",
        "## ü§ñ Section 3 ‚Äî Model Initialisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "model-init",
      "metadata": {
        "id": "model-init"
      },
      "outputs": [],
      "source": [
        "GEMINI_MODEL_NAME = \"gemini-1.5-flash\"\n",
        "\n",
        "gemini_model = genai.GenerativeModel(\n",
        "    model_name        = GEMINI_MODEL_NAME,\n",
        "    generation_config = genai.types.GenerationConfig(\n",
        "        temperature       = 0.2,   # Slight variability for natural phrasing\n",
        "        max_output_tokens = 80,    # One short question only\n",
        "    ),\n",
        "    safety_settings = [\n",
        "        {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_ONLY_HIGH\"},\n",
        "        {\"category\": \"HARM_CATEGORY_HARASSMENT\",        \"threshold\": \"BLOCK_ONLY_HIGH\"},\n",
        "        {\"category\": \"HARM_CATEGORY_HATE_SPEECH\",       \"threshold\": \"BLOCK_ONLY_HIGH\"},\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Gemini model ready: {GEMINI_MODEL_NAME}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-4",
      "metadata": {
        "id": "section-4"
      },
      "source": [
        "---\n",
        "## üóÇÔ∏è Section 4 ‚Äî Data Structures\n",
        "\n",
        "Three simple dataclasses carry all the state needed to select the next question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "data-structures",
      "metadata": {
        "id": "data-structures"
      },
      "outputs": [],
      "source": [
        "class ConversationStage(str, Enum):\n",
        "    EARLY      = \"early\"       # First 1‚Äì3 questions\n",
        "    MID        = \"mid\"         # Filling in known gaps\n",
        "    ESCALATION = \"escalation\"  # Red flag follow-up\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class PatientState:\n",
        "    \"\"\"\n",
        "    Structured patient information ‚Äî produced by Model B.\n",
        "    Populate only what is known; leave unknown fields empty.\n",
        "    \"\"\"\n",
        "    age                     : Optional[int]   = None\n",
        "    chief_complaint         : str             = \"\"\n",
        "    duration                : str             = \"\"\n",
        "    severity                : str             = \"\"   # patient's own words\n",
        "    body_part               : str             = \"\"\n",
        "    associated_symptoms     : List[str]       = field(default_factory=list)\n",
        "    red_flags_present       : Optional[bool]  = None\n",
        "    additional_observations : str             = \"\"\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ConversationContext:\n",
        "    \"\"\"\n",
        "    Running record of the dialogue so far.\n",
        "    \"\"\"\n",
        "    questions_asked  : List[str]             = field(default_factory=list)\n",
        "    patient_answers  : List[str]             = field(default_factory=list)\n",
        "    stage            : ConversationStage     = ConversationStage.EARLY\n",
        "\n",
        "    def add_turn(self, question: str, answer: str):\n",
        "        \"\"\"Record one Q&A exchange and advance the conversation stage.\"\"\"\n",
        "        self.questions_asked.append(question)\n",
        "        self.patient_answers.append(answer)\n",
        "        n = len(self.questions_asked)\n",
        "        if n <= 2:\n",
        "            self.stage = ConversationStage.EARLY\n",
        "        elif n <= 6:\n",
        "            self.stage = ConversationStage.MID\n",
        "        else:\n",
        "            self.stage = ConversationStage.ESCALATION\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class QuestionResult:\n",
        "    \"\"\"Output of one Model C call.\"\"\"\n",
        "    question         : str    # The question to speak to the patient\n",
        "    stage            : str    # Conversation stage at time of selection\n",
        "    red_flag_active  : bool   # Whether red flag mode influenced selection\n",
        "\n",
        "\n",
        "print(\"‚úÖ Data structures defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-5",
      "metadata": {
        "id": "section-5"
      },
      "source": [
        "---\n",
        "## üìã Section 5 ‚Äî Coverage Checklist & Clinical Rules\n",
        "\n",
        "The checklist defines what **must eventually be covered**. The rules guide **prioritisation order** ‚Äî they are soft constraints, not rigid scripts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rules",
      "metadata": {
        "id": "rules"
      },
      "outputs": [],
      "source": [
        "# ‚îÄ‚îÄ Required coverage categories ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# These topics must be addressed before the consultation is considered complete.\n",
        "# Order is flexible; coverage is not.\n",
        "COVERAGE_CHECKLIST = [\n",
        "    \"severity or intensity of the main symptom\",\n",
        "    \"when the symptom started or how long it has been present\",\n",
        "    \"whether the symptom is getting better, worse, or staying the same\",\n",
        "    \"any other symptoms alongside the main one\",\n",
        "    \"whether the symptom affects the patient's daily activities\",\n",
        "    \"any relevant medical history or known conditions\",\n",
        "]\n",
        "\n",
        "# ‚îÄ‚îÄ Red flag follow-up questions (activated when red_flags_present = True) ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "RED_FLAG_FOLLOWUPS = [\n",
        "    \"Is the patient currently able to breathe comfortably?\",\n",
        "    \"Has the patient lost consciousness or felt faint?\",\n",
        "    \"Is there any unusual bleeding?\",\n",
        "    \"Is the patient able to move all limbs normally?\",\n",
        "    \"Is the patient in severe pain right now?\",\n",
        "]\n",
        "\n",
        "# ‚îÄ‚îÄ Soft prioritisation rules (passed to the model as guidance) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "PRIORITISATION_RULES = \"\"\"\\\n",
        "- If the main symptom involves pain and severity is unknown ‚Üí ask about severity first.\n",
        "- If severity is high or a red flag is present ‚Üí immediately cover red-flag screening questions.\n",
        "- If the symptom is acute (sudden onset) ‚Üí prioritise onset time and progression.\n",
        "- If the symptom is respiratory or cardiac ‚Üí prioritise breathing and consciousness.\n",
        "- If the patient is elderly (65+) or very young (under 5) ‚Üí treat all gaps as higher priority.\n",
        "- Otherwise ‚Üí follow the coverage checklist in a natural, conversational order.\n",
        "\"\"\"\n",
        "\n",
        "print(\"‚úÖ Coverage checklist and clinical rules defined.\")\n",
        "print(f\"   Coverage categories : {len(COVERAGE_CHECKLIST)}\")\n",
        "print(f\"   Red flag follow-ups : {len(RED_FLAG_FOLLOWUPS)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-6",
      "metadata": {
        "id": "section-6"
      },
      "source": [
        "---\n",
        "## üìù Section 6 ‚Äî Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "prompt",
      "metadata": {
        "id": "prompt"
      },
      "outputs": [],
      "source": [
        "SYSTEM_PROMPT = \"\"\"\\\n",
        "You are a clinical question-selection assistant in a voice-based hospital pre-consultation system.\n",
        "Your only job is to choose the single best next question to ask the patient.\n",
        "\n",
        "=== HARD RULES (NEVER VIOLATE) ===\n",
        "- Output ONE question only ‚Äî nothing else.\n",
        "- Do NOT diagnose, label, or suggest any medical condition.\n",
        "- Do NOT offer reassurance, advice, or interpretation.\n",
        "- Do NOT repeat a question already asked.\n",
        "- Do NOT ask multiple questions at once.\n",
        "- The question must be short, clear, and natural to say out loud.\n",
        "- Write in plain English (or Kinyarwanda if that is the patient's language).\n",
        "- Output the question text only ‚Äî no explanation, no prefix, no punctuation other than the question mark.\n",
        "\n",
        "=== YOUR GOAL ===\n",
        "Select the question that best reduces clinical uncertainty given what is already known,\n",
        "following the prioritisation rules and coverage checklist provided.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def build_prompt(state: PatientState, context: ConversationContext) -> str:\n",
        "    \"\"\"Build the user-turn prompt from current patient state and conversation context.\"\"\"\n",
        "\n",
        "    # Format known patient information\n",
        "    known = []\n",
        "    if state.age:                   known.append(f\"Age: {state.age}\")\n",
        "    if state.chief_complaint:       known.append(f\"Main symptom: {state.chief_complaint}\")\n",
        "    if state.duration:              known.append(f\"Duration: {state.duration}\")\n",
        "    if state.severity:              known.append(f\"Severity: {state.severity}\")\n",
        "    if state.body_part:             known.append(f\"Body part: {state.body_part}\")\n",
        "    if state.associated_symptoms:   known.append(f\"Other symptoms: {', '.join(state.associated_symptoms)}\")\n",
        "    if state.additional_observations: known.append(f\"Notes: {state.additional_observations}\")\n",
        "\n",
        "    # Identify what is still missing from coverage checklist\n",
        "    filled_fields = set(f.lower() for f in [\n",
        "        state.severity, state.duration, state.chief_complaint,\n",
        "        str(state.associated_symptoms), state.additional_observations\n",
        "    ] if f)\n",
        "    missing = [\n",
        "        item for item in COVERAGE_CHECKLIST\n",
        "        if not any(keyword in filled_fields for keyword in item.split()[:2])\n",
        "    ]\n",
        "\n",
        "    # Format conversation history\n",
        "    history = \"\"\n",
        "    if context.questions_asked:\n",
        "        pairs = [\n",
        "            f\"  Q: {q}\\n  A: {a}\"\n",
        "            for q, a in zip(context.questions_asked, context.patient_answers)\n",
        "        ]\n",
        "        history = \"\\n\".join(pairs)\n",
        "    else:\n",
        "        history = \"  (none yet)\"\n",
        "\n",
        "    # Red flag instructions\n",
        "    red_flag_block = \"\"\n",
        "    if state.red_flags_present:\n",
        "        red_flag_block = f\"\"\"\n",
        "=== ‚ö†Ô∏è RED FLAG ACTIVE ===\n",
        "A red flag has been detected. Prioritise these follow-up questions next (pick the most relevant one not yet asked):\n",
        "{chr(10).join(f'- {q}' for q in RED_FLAG_FOLLOWUPS)}\n",
        "\"\"\"\n",
        "\n",
        "    return f\"\"\"\\\n",
        "=== PATIENT STATE (known so far) ===\n",
        "{chr(10).join(f'- {k}' for k in known) or '- (nothing known yet)'}\n",
        "\n",
        "=== MISSING INFORMATION (still to cover) ===\n",
        "{chr(10).join(f'- {m}' for m in missing) or '- (all core topics covered)'}\n",
        "\n",
        "=== CONVERSATION HISTORY ===\n",
        "{history}\n",
        "\n",
        "=== CONVERSATION STAGE ===\n",
        "{context.stage.value}\n",
        "{red_flag_block}\n",
        "=== PRIORITISATION RULES ===\n",
        "{PRIORITISATION_RULES}\n",
        "\n",
        "=== TASK ===\n",
        "Output the single best next question to ask the patient. Question only. Nothing else.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "print(\"‚úÖ Prompt builder defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-7",
      "metadata": {
        "id": "section-7"
      },
      "source": [
        "---\n",
        "## üöÄ Section 7 ‚Äî Core Question Selection Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "core-function",
      "metadata": {
        "id": "core-function"
      },
      "outputs": [],
      "source": [
        "def select_next_question(\n",
        "    state   : PatientState,\n",
        "    context : ConversationContext,\n",
        "    verbose : bool = True,\n",
        ") -> QuestionResult:\n",
        "    \"\"\"\n",
        "    Select the single best next question to ask the patient.\n",
        "\n",
        "    Args:\n",
        "        state   : Current structured patient state (from Model B).\n",
        "        context : Running conversation context.\n",
        "        verbose : Print the selected question.\n",
        "\n",
        "    Returns:\n",
        "        QuestionResult with the question and metadata.\n",
        "    \"\"\"\n",
        "    prompt = build_prompt(state, context)\n",
        "\n",
        "    # One-shot call ‚Äî no chat history needed; each call is self-contained\n",
        "    chat = gemini_model.start_chat(history=[\n",
        "        {\"role\": \"user\",  \"parts\": [SYSTEM_PROMPT]},\n",
        "        {\"role\": \"model\", \"parts\": [\"Understood. I will output one question only, with no diagnosis, advice, or extra text.\"]},\n",
        "    ])\n",
        "    response = chat.send_message(prompt)\n",
        "    raw      = response.text.strip()\n",
        "\n",
        "    # Clean up: strip leading labels like \"Question:\" if Gemini adds them\n",
        "    question = re.sub(r\"^(question|next question|q)[:\\-]?\\s*\", \"\", raw, flags=re.IGNORECASE).strip()\n",
        "    # Ensure it ends with a question mark\n",
        "    if question and not question.endswith(\"?\"):\n",
        "        question += \"?\"\n",
        "\n",
        "    result = QuestionResult(\n",
        "        question        = question,\n",
        "        stage           = context.stage.value,\n",
        "        red_flag_active = bool(state.red_flags_present),\n",
        "    )\n",
        "\n",
        "    if verbose:\n",
        "        flag_marker = \"üö®\" if result.red_flag_active else \"üí¨\"\n",
        "        print(f\"\\n{flag_marker} [{result.stage.upper()}] Next question:\")\n",
        "        print(f\"   ‚ñ∂ {result.question}\")\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "print(\"‚úÖ select_next_question() defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-8",
      "metadata": {
        "id": "section-8"
      },
      "source": [
        "---\n",
        "## üîó Section 8 ‚Äî Full Pipeline: Model B ‚Üí Model C\n",
        "\n",
        "This function accepts Model B's output dict directly and runs one question-selection step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "integration",
      "metadata": {
        "id": "integration"
      },
      "outputs": [],
      "source": [
        "def run_from_model_b(\n",
        "    model_b_output : dict,\n",
        "    context        : ConversationContext,\n",
        "    verbose        : bool = True,\n",
        ") -> QuestionResult:\n",
        "    \"\"\"\n",
        "    Accepts Model B's extraction dict and runs Model C question selection.\n",
        "\n",
        "    Args:\n",
        "        model_b_output : Dict returned by Model B's extract_clinical_information().\n",
        "        context        : Running ConversationContext (maintained by the caller).\n",
        "        verbose        : Print progress.\n",
        "\n",
        "    Returns:\n",
        "        QuestionResult.\n",
        "    \"\"\"\n",
        "    ext = model_b_output.get(\"extraction_dict\", {})\n",
        "\n",
        "    state = PatientState(\n",
        "        chief_complaint         = ext.get(\"chief_complaint\", \"\"),\n",
        "        duration                = ext.get(\"duration\", \"\"),\n",
        "        severity                = ext.get(\"severity\", \"\"),\n",
        "        body_part               = ext.get(\"body_part\", \"\"),\n",
        "        associated_symptoms     = ext.get(\"associated_symptoms\", []),\n",
        "        red_flags_present       = ext.get(\"red_flags_present\"),\n",
        "        additional_observations = ext.get(\"additional_observations\", \"\"),\n",
        "    )\n",
        "\n",
        "    if verbose:\n",
        "        print(\"üîó Model B ‚Üí Model C\")\n",
        "        print(f\"   Complaint : {state.chief_complaint or '(unknown)'}\")\n",
        "        print(f\"   Red flag  : {state.red_flags_present}\")\n",
        "        print(f\"   Stage     : {context.stage.value}\")\n",
        "\n",
        "    return select_next_question(state, context, verbose=verbose)\n",
        "\n",
        "\n",
        "print(\"‚úÖ run_from_model_b() defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-9",
      "metadata": {
        "id": "section-9"
      },
      "source": [
        "---\n",
        "## üîÑ Section 9 ‚Äî Simulated Conversation Loop\n",
        "\n",
        "This simulates a full pre-consultation session. In production, each `patient_answer` comes from Model A (voice ‚Üí text). Here we provide answers manually to show the loop working."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "conversation-loop",
      "metadata": {
        "id": "conversation-loop"
      },
      "outputs": [],
      "source": [
        "def run_conversation(\n",
        "    state          : PatientState,\n",
        "    simulated_answers : List[str],   # In production: replaced by Model A output\n",
        "    max_questions  : int = 6,\n",
        ") -> List[dict]:\n",
        "    \"\"\"\n",
        "    Simulate a full Model C conversation loop.\n",
        "\n",
        "    Args:\n",
        "        state             : Initial PatientState (from Model B).\n",
        "        simulated_answers : Pre-written patient answers (for testing).\n",
        "        max_questions     : Stop after this many questions.\n",
        "\n",
        "    Returns:\n",
        "        List of turn dicts {turn, question, answer, stage}.\n",
        "    \"\"\"\n",
        "    context = ConversationContext()\n",
        "    log     = []\n",
        "\n",
        "    print(\"\\n\" + \"‚ïê\" * 60)\n",
        "    print(\" üè•  SIMULATED PRE-CONSULTATION SESSION\")\n",
        "    print(\"‚ïê\" * 60)\n",
        "    print(f\"  Chief complaint : {state.chief_complaint or '(not yet known)'}\")\n",
        "    print(f\"  Red flag active : {state.red_flags_present}\")\n",
        "    print()\n",
        "\n",
        "    for turn in range(1, max_questions + 1):\n",
        "        print(f\"  ‚îÄ‚îÄ Turn {turn} {'‚îÄ' * 46}\")\n",
        "\n",
        "        result = select_next_question(state, context, verbose=True)\n",
        "\n",
        "        # In production: answer = Model A transcription of patient voice response\n",
        "        answer = simulated_answers[turn - 1] if turn <= len(simulated_answers) else \"I don't know.\"\n",
        "        print(f\"   Patient: {answer}\")\n",
        "\n",
        "        context.add_turn(result.question, answer)\n",
        "        log.append({\n",
        "            \"turn\"     : turn,\n",
        "            \"question\" : result.question,\n",
        "            \"answer\"   : answer,\n",
        "            \"stage\"    : result.stage,\n",
        "        })\n",
        "\n",
        "    print(\"\\n\" + \"‚ïê\" * 60)\n",
        "    print(f\" ‚úÖ  Session complete ‚Äî {len(log)} questions asked.\")\n",
        "    print(\"‚ïê\" * 60)\n",
        "    return log\n",
        "\n",
        "\n",
        "print(\"‚úÖ Conversation loop defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-10",
      "metadata": {
        "id": "section-10"
      },
      "source": [
        "---\n",
        "## üß™ Section 10 ‚Äî Test Cases\n",
        "\n",
        "Three scenarios: standard English, Kinyarwanda, and a red flag case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "test-english",
      "metadata": {
        "id": "test-english"
      },
      "outputs": [],
      "source": [
        "# ‚îÄ‚îÄ Test Case 1: English ‚Äî headache ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "state_1 = PatientState(\n",
        "    age             = 34,\n",
        "    chief_complaint = \"headache\",\n",
        "    body_part       = \"head\",\n",
        "    # severity, duration, associated_symptoms not yet known\n",
        ")\n",
        "\n",
        "answers_1 = [\n",
        "    \"It started this morning.\",\n",
        "    \"About a seven out of ten.\",\n",
        "    \"It's getting worse.\",\n",
        "    \"I feel a bit nauseous.\",\n",
        "    \"I can't really focus on work.\",\n",
        "    \"No, I don't have any known conditions.\",\n",
        "]\n",
        "\n",
        "log_1 = run_conversation(state_1, answers_1, max_questions=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "test-kinyarwanda",
      "metadata": {
        "id": "test-kinyarwanda"
      },
      "outputs": [],
      "source": [
        "# ‚îÄ‚îÄ Test Case 2: Kinyarwanda ‚Äî abdominal pain ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "state_2 = PatientState(\n",
        "    chief_complaint = \"ububabare mu nda\",   # stomach pain in Kinyarwanda\n",
        "    body_part       = \"nda\",               # abdomen\n",
        "    # other fields unknown\n",
        ")\n",
        "\n",
        "answers_2 = [\n",
        "    \"Kuva ejo.\",                     # Since yesterday\n",
        "    \"Ni uburemere cyane.\",           # It is very heavy/severe\n",
        "    \"Nshaka kuruka ariko sinabikora.\",# Feel like vomiting but haven't\n",
        "    \"Sinashye neza.\",                # Didn't sleep well\n",
        "    \"Oya, nta ndwara nsanzwe.\",      # No known conditions\n",
        "]\n",
        "\n",
        "log_2 = run_conversation(state_2, answers_2, max_questions=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "test-redflag",
      "metadata": {
        "id": "test-redflag"
      },
      "outputs": [],
      "source": [
        "# ‚îÄ‚îÄ Test Case 3: English ‚Äî red flag (chest pain) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "state_3 = PatientState(\n",
        "    age               = 58,\n",
        "    chief_complaint   = \"chest pain\",\n",
        "    duration          = \"2 hours\",\n",
        "    body_part         = \"chest\",\n",
        "    red_flags_present = True,           # Already flagged by Model B\n",
        ")\n",
        "\n",
        "answers_3 = [\n",
        "    \"Yes, it's hard to breathe deeply.\",\n",
        "    \"No, I haven't fainted but I feel dizzy.\",\n",
        "    \"The pain goes to my left arm.\",\n",
        "    \"Eight out of ten.\",\n",
        "]\n",
        "\n",
        "log_3 = run_conversation(state_3, answers_3, max_questions=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-11",
      "metadata": {
        "id": "section-11"
      },
      "source": [
        "---\n",
        "## üì§ Section 11 ‚Äî Export Session Log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "export",
      "metadata": {
        "id": "export"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "from google.colab import files\n",
        "\n",
        "def export_session(log: list, label: str = \"session\"):\n",
        "    \"\"\"Save a conversation log to JSON and download it.\"\"\"\n",
        "    export = {\n",
        "        \"timestamp\"      : datetime.datetime.now().isoformat(),\n",
        "        \"model\"          : GEMINI_MODEL_NAME,\n",
        "        \"total_questions\": len(log),\n",
        "        \"turns\"          : log,\n",
        "    }\n",
        "    path = f\"model_c_{label}.json\"\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(export, f, ensure_ascii=False, indent=2)\n",
        "    print(f\"‚úÖ Saved: {path}\")\n",
        "    files.download(path)\n",
        "\n",
        "\n",
        "# Export whichever session log you want:\n",
        "# export_session(log_1, \"english_headache\")\n",
        "# export_session(log_2, \"kinyarwanda_abdomen\")\n",
        "# export_session(log_3, \"redflag_chest_pain\")\n",
        "\n",
        "print(\"‚úÖ Export function ready. Uncomment the line for the session you want to download.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "notes",
      "metadata": {
        "id": "notes"
      },
      "source": [
        "---\n",
        "## üìù Notes\n",
        "\n",
        "| Topic | Detail |\n",
        "|---|---|\n",
        "| **Temperature = 0.2** | Allows natural phrasing variation while keeping question focus consistent. Lower to 0.0 for fully deterministic output. |\n",
        "| **max_output_tokens = 80** | Hard ceiling that forces Gemini to output a single short question and nothing more. |\n",
        "| **Red flag mode** | When `red_flags_present = True`, the red flag follow-up list is injected into the prompt and takes priority over the standard checklist. |\n",
        "| **Kinyarwanda** | The model is instructed to match the patient's language. For consistent Kinyarwanda output, verify with native speakers and refine phrasings in the prompt or simulated answers. |\n",
        "| **Coverage vs. order** | The checklist guarantees eventual topic coverage; the model chooses the most appropriate order per context. |\n",
        "| **Production wiring** | Replace `simulated_answers` in `run_conversation()` with live Model A transcriptions to go real-time. |\n",
        "| **Data path** | Update file paths for training transcripts and annotated flows once finalised. |"
      ]
    }
  ]
}