{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": []
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# ðŸ—£ï¸ Model E â€” Patient-Facing Guidance & Next-Steps\n",
    "\n",
    "**Purpose:** Translate system decisions into calm, clear, non-diagnostic guidance for the patient. Model E never reasons medically â€” it only communicates.\n",
    "\n",
    "| | |\n",
    "|---|---|\n",
    "| **Input** | Structured symptoms (Model B) + priority level (Model D) + key flags |\n",
    "| **Output** | A patient-facing message â€” plain, calm, voice-ready |\n",
    "| **Model** | Google Gemini AI (controlled generation, no fine-tuning) |\n",
    "| **Audience** | The patient â€” directly |\n",
    "\n",
    "### Pipeline Position\n",
    "```\n",
    "Model B + Model D Output              Model E Output\n",
    "(Symptoms + Priority)\n",
    "         â”‚\n",
    "         â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  priority: HIGH         â”‚     â”‚ \"Your symptoms need prompt attention. â”‚\n",
    "â”‚  symptoms: chest pain   â”‚â”€â”€â”€â”€â–ºâ”‚  Please proceed to Emergency, Desk 3. â”‚\n",
    "â”‚  red_flags: True        â”‚     â”‚  A healthcare professional will see   â”‚\n",
    "â”‚  context answers        â”‚     â”‚  you shortly...\"                      â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                              â”‚\n",
    "                                              â–¼\n",
    "                                     Spoken to patient\n",
    "                                     Displayed on screen\n",
    "```\n",
    "\n",
    "> âš ï¸ **Hard Rules:** Never names a disease. Never suggests a cause. Never gives medical advice. Never overrides the system's priority decision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ“¦ Section 1 â€” Install & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U google-generativeai\n",
    "\n",
    "import json\n",
    "import re\n",
    "import datetime\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from typing import List, Optional\n",
    "\n",
    "import google.generativeai as genai\n",
    "from google.colab import userdata\n",
    "\n",
    "print(\"âœ… Dependencies ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ”‘ Section 2 â€” API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "api-key",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your key via: left sidebar â†’ ðŸ”‘ Secrets â†’ \"GEMINI_API_KEY\"\n",
    "try:\n",
    "    genai.configure(api_key=userdata.get(\"GEMINI_API_KEY\"))\n",
    "    print(\"âœ… API key loaded from Colab Secrets.\")\n",
    "except Exception:\n",
    "    genai.configure(api_key=\"YOUR_API_KEY_HERE\")  # fallback only\n",
    "    print(\"âš ï¸  API key set manually. Use Colab Secrets for security.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ¤– Section 3 â€” Model Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-init",
   "metadata": {},
   "outputs": [],
   "source": [
    "GEMINI_MODEL_NAME = \"gemini-1.5-flash\"\n",
    "\n",
    "gemini_model = genai.GenerativeModel(\n",
    "    model_name        = GEMINI_MODEL_NAME,\n",
    "    generation_config = genai.types.GenerationConfig(\n",
    "        temperature       = 0.3,   # Slight warmth for natural patient-friendly phrasing\n",
    "        max_output_tokens = 180,   # Keep messages short â€” 3 to 5 sentences max\n",
    "    ),\n",
    "    safety_settings = [\n",
    "        {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_ONLY_HIGH\"},\n",
    "        {\"category\": \"HARM_CATEGORY_HARASSMENT\",        \"threshold\": \"BLOCK_ONLY_HIGH\"},\n",
    "        {\"category\": \"HARM_CATEGORY_HATE_SPEECH\",       \"threshold\": \"BLOCK_ONLY_HIGH\"},\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"âœ… Gemini initialised: {GEMINI_MODEL_NAME} | temperature=0.3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-4",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ—‚ï¸ Section 4 â€” Data Structures\n",
    "\n",
    "Two simple dataclasses: one for input (compiled from Models B and D), one for output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-structures",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GuidanceInput:\n",
    "    \"\"\"\n",
    "    Everything Model E needs. Compiled from Model B and Model D.\n",
    "    Model E does not re-infer anything â€” it only reads what is here.\n",
    "    \"\"\"\n",
    "    # From Model D\n",
    "    priority           : str             = \"MEDIUM\"   # HIGH | MEDIUM | LOW\n",
    "    # From Model B\n",
    "    chief_complaint    : str             = \"\"\n",
    "    duration           : str             = \"\"\n",
    "    severity           : str             = \"\"\n",
    "    associated_symptoms: List[str]       = field(default_factory=list)\n",
    "    red_flags_present  : Optional[bool]  = None\n",
    "    # Optional context\n",
    "    patient_language   : str             = \"english\"  # 'english' | 'kinyarwanda'\n",
    "    location_context   : str             = \"\"         # e.g. \"Emergency, Desk 3\"\n",
    "    uncertainty_flag   : bool            = False      # True if data was sparse/incomplete\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GuidanceOutput:\n",
    "    \"\"\"\n",
    "    Model E's patient-facing message.\n",
    "    \"\"\"\n",
    "    message            : str   # The message to speak or display to the patient\n",
    "    priority_used      : str   # Priority level that drove the message\n",
    "    language           : str   # Language the message was generated in\n",
    "\n",
    "\n",
    "print(\"âœ… Data structures defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-5",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ“‹ Section 5 â€” Safe Fallback Templates\n",
    "\n",
    "If the API call fails, or if data is too sparse, the system falls back to pre-approved template messages. Templates are audited, deterministic, and always safe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "templates",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Fallback templates â€” used when API fails or data is insufficient â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# These are plain, safe, pre-approved messages for each priority level.\n",
    "FALLBACK_TEMPLATES = {\n",
    "    \"HIGH\": (\n",
    "        \"Your symptoms need prompt medical attention. \"\n",
    "        \"Please proceed to the nearest staff member or emergency desk immediately. \"\n",
    "        \"If you feel worse at any time, notify a staff member right away.\"\n",
    "    ),\n",
    "    \"MEDIUM\": (\n",
    "        \"Thank you for sharing your symptoms. \"\n",
    "        \"A healthcare professional will see you shortly. \"\n",
    "        \"Please take a seat and wait to be called. \"\n",
    "        \"If your condition changes or worsens, please let a staff member know.\"\n",
    "    ),\n",
    "    \"LOW\": (\n",
    "        \"Thank you. Your information has been recorded. \"\n",
    "        \"Please wait in the waiting area and you will be called when it is your turn. \"\n",
    "        \"If you have any concerns while waiting, feel free to speak to a staff member.\"\n",
    "    ),\n",
    "    \"UNKNOWN\": (\n",
    "        \"Thank you for your information. \"\n",
    "        \"Please wait while we complete the next step. \"\n",
    "        \"A staff member will assist you shortly.\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "# â”€â”€ Kinyarwanda fallback templates â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "FALLBACK_TEMPLATES_KIN = {\n",
    "    \"HIGH\": (\n",
    "        \"Ibimenyetso byawe bisaba ubuvuzi bwihutirwa. \"\n",
    "        \"Mwihangane, muzajya guturikirwa na muganga vuba vuba. \"\n",
    "        \"Niba imiterere yawe ihinduka cyangwa irushaho gukomera, menyesha umukozi vuba.\"\n",
    "    ),\n",
    "    \"MEDIUM\": (\n",
    "        \"Murakoze gushiraho ibimenyetso byanyu. \"\n",
    "        \"Muganga azababona vuba. \"\n",
    "        \"Mwicare mukomeze gutegereza. \"\n",
    "        \"Niba hari impinduka, menyesha umukozi.\"\n",
    "    ),\n",
    "    \"LOW\": (\n",
    "        \"Murakoze. Amakuru yanyu yanditswe. \"\n",
    "        \"Mwicare mukomeze gutegereza kugeza igihe muzahamagalwa. \"\n",
    "        \"Niba mufite ibibazo, mubwire umukozi.\"\n",
    "    ),\n",
    "    \"UNKNOWN\": (\n",
    "        \"Murakoze gutanga amakuru. \"\n",
    "        \"Mwihangane turangije intambwe ikurikira. \"\n",
    "        \"Umukozi azabafasha vuba.\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "print(\"âœ… Fallback templates loaded (English + Kinyarwanda).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-6",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ“ Section 6 â€” Prompt\n",
    "\n",
    "The system prompt enforces all hard constraints once. The user prompt injects the system decision at runtime â€” the model explains it, never overrides it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prompt",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\\\n",
    "You are a healthcare system assistant in a hospital.\n",
    "Your only job is to explain the next steps to a patient using the system's decision.\n",
    "\n",
    "=== HARD RULES (NEVER VIOLATE) ===\n",
    "- Do NOT name any disease, condition, or diagnosis.\n",
    "- Do NOT suggest a cause for the patient's symptoms.\n",
    "- Do NOT give any medical advice or treatment suggestions.\n",
    "- Do NOT change, escalate, or downgrade the priority level given to you.\n",
    "- Do NOT use medical terminology the patient would not understand.\n",
    "- Keep the message calm, clear, and reassuring.\n",
    "- Write 3 to 5 short sentences only.\n",
    "- If any key information is missing, do not guess â€” defer with:\n",
    "  \"Please wait while we complete the next step.\"\n",
    "- Output the patient message only â€” no labels, no explanations, no extra text.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# â”€â”€ Few-shot examples (template-LLM hybrid approach) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# These are injected as conversation history so Gemini learns the expected\n",
    "# style and tone before seeing the real input.\n",
    "FEW_SHOT_HISTORY = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"parts\": [\"\"\"\\\n",
    "Priority: HIGH\n",
    "Chief complaint: chest pain\n",
    "Duration: 2 hours\n",
    "Severity: severe\n",
    "Red flag: yes\n",
    "Location: Emergency, Desk 3\n",
    "Language: english\"\"\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"model\",\n",
    "        \"parts\": [\"\"\"\\\n",
    "Your symptoms need prompt medical attention. \\\n",
    "Please proceed to Emergency, Desk 3 right away. \\\n",
    "A healthcare professional will see you shortly. \\\n",
    "If you feel worse at any time, please notify a staff member immediately.\"\"\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"parts\": [\"\"\"\\\n",
    "Priority: MEDIUM\n",
    "Chief complaint: headache\n",
    "Duration: 2 days\n",
    "Severity: moderate\n",
    "Red flag: no\n",
    "Location: Waiting Area B\n",
    "Language: english\"\"\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"model\",\n",
    "        \"parts\": [\"\"\"\\\n",
    "Thank you for sharing your information. \\\n",
    "Please take a seat in Waiting Area B and a healthcare professional will see you soon. \\\n",
    "We have noted your symptoms and you will be called when it is your turn. \\\n",
    "If your condition changes or worsens while you wait, please let a staff member know.\"\"\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"parts\": [\"\"\"\\\n",
    "Priority: LOW\n",
    "Chief complaint: sore throat\n",
    "Duration: 3 days\n",
    "Severity: mild\n",
    "Red flag: no\n",
    "Location: General Outpatient\n",
    "Language: english\"\"\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"model\",\n",
    "        \"parts\": [\"\"\"\\\n",
    "Thank you. Your information has been recorded. \\\n",
    "Please wait in the General Outpatient area and you will be called when it is your turn. \\\n",
    "If you have any concerns while waiting, please speak to a staff member.\"\"\"\n",
    "        ]\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "def build_prompt(inp: GuidanceInput) -> str:\n",
    "    \"\"\"Build the user-turn prompt from a GuidanceInput.\"\"\"\n",
    "    lines = [\n",
    "        f\"Priority: {inp.priority}\",\n",
    "        f\"Chief complaint: {inp.chief_complaint or 'not specified'}\",\n",
    "        f\"Duration: {inp.duration or 'not specified'}\",\n",
    "        f\"Severity: {inp.severity or 'not specified'}\",\n",
    "        f\"Red flag: {'yes' if inp.red_flags_present else 'no' if inp.red_flags_present is False else 'unknown'}\",\n",
    "        f\"Other symptoms: {', '.join(inp.associated_symptoms) if inp.associated_symptoms else 'none'}\",\n",
    "        f\"Location: {inp.location_context or 'not specified'}\",\n",
    "        f\"Language: {inp.patient_language}\",\n",
    "    ]\n",
    "    if inp.uncertainty_flag:\n",
    "        lines.append(\"Note: Some patient data was incomplete. Keep the message general.\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "print(\"âœ… Prompt and few-shot examples defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-7",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ›¡ï¸ Section 7 â€” Output Safety Check\n",
    "\n",
    "A lightweight post-generation scan that flags if the model has accidentally included forbidden content (diagnostic language, medical advice, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "safety-check",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words and phrases that must never appear in a patient-facing message\n",
    "FORBIDDEN_PHRASES = [\n",
    "    # Diagnostic language\n",
    "    \"you have\", \"you may have\", \"this is\", \"this could be\", \"this looks like\",\n",
    "    \"likely\", \"probably\", \"possibly\", \"diagnosis\", \"condition is\",\n",
    "    # Disease names (a sample â€” extend as needed)\n",
    "    \"heart attack\", \"stroke\", \"cancer\", \"infection\", \"diabetes\",\n",
    "    \"hypertension\", \"pneumonia\", \"appendicitis\",\n",
    "    # Medical advice\n",
    "    \"take\", \"medication\", \"painkiller\", \"rest\", \"drink water\",\n",
    "    \"avoid\", \"do not eat\", \"apply\",\n",
    "    # Urgency inflation\n",
    "    \"life-threatening\", \"critical\", \"emergency\", \"dying\",\n",
    "]\n",
    "\n",
    "\n",
    "def safety_check(message: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Scan the generated message for forbidden content.\n",
    "\n",
    "    Returns:\n",
    "        (is_safe: bool, violations: list of matched phrases)\n",
    "    \"\"\"\n",
    "    lowered    = message.lower()\n",
    "    violations = [p for p in FORBIDDEN_PHRASES if p in lowered]\n",
    "    return len(violations) == 0, violations\n",
    "\n",
    "\n",
    "print(\"âœ… Safety check defined.\")\n",
    "print(f\"   Monitoring {len(FORBIDDEN_PHRASES)} forbidden phrases.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-8",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸš€ Section 8 â€” Core Guidance Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "core-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_patient_guidance(\n",
    "    guidance_input : GuidanceInput,\n",
    "    verbose        : bool = True,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Generate a patient-facing next-steps message.\n",
    "\n",
    "    Args:\n",
    "        guidance_input : Compiled input from Models B and D.\n",
    "        verbose        : Print the message.\n",
    "\n",
    "    Returns:\n",
    "        dict with keys: output (GuidanceOutput), message, safe, violations, success, error\n",
    "    \"\"\"\n",
    "    priority = guidance_input.priority.upper()\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"â•\" * 58)\n",
    "        print(\" ðŸ—£ï¸   PATIENT GUIDANCE â€” MODEL E\")\n",
    "        print(\"â•\" * 58)\n",
    "        print(f\"  Priority   : {priority}\")\n",
    "        print(f\"  Complaint  : {guidance_input.chief_complaint or '(not specified)'}\")\n",
    "        print(f\"  Language   : {guidance_input.patient_language}\")\n",
    "        print(f\"  Red flag   : {guidance_input.red_flags_present}\")\n",
    "        print(f\"  Uncertain  : {guidance_input.uncertainty_flag}\")\n",
    "\n",
    "    # â”€â”€ Hard fallback: uncertainty flag or unknown priority â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    if guidance_input.uncertainty_flag or priority not in (\"HIGH\", \"MEDIUM\", \"LOW\"):\n",
    "        templates = FALLBACK_TEMPLATES_KIN if guidance_input.patient_language == \"kinyarwanda\" \\\n",
    "                    else FALLBACK_TEMPLATES\n",
    "        message = templates.get(\"UNKNOWN\")\n",
    "        if verbose:\n",
    "            print(\"\\n  âš ï¸  Uncertainty flag set â€” using safe fallback message.\")\n",
    "            _print_message(message)\n",
    "        output = GuidanceOutput(message=message, priority_used=priority,\n",
    "                                language=guidance_input.patient_language)\n",
    "        return {\"output\": output, \"message\": message, \"safe\": True,\n",
    "                \"violations\": [], \"success\": True, \"error\": None}\n",
    "\n",
    "    # â”€â”€ Call Gemini â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    try:\n",
    "        if verbose:\n",
    "            print(\"\\n  ðŸ”„ Calling Gemini API...\")\n",
    "\n",
    "        # Prime the chat with system prompt + few-shot examples\n",
    "        history = [{\"role\": \"user\", \"parts\": [SYSTEM_PROMPT]},\n",
    "                   {\"role\": \"model\", \"parts\": [\"Understood. I will write only the patient message â€” calm, clear, and with no diagnosis or medical advice.\"]}]\n",
    "        history += FEW_SHOT_HISTORY\n",
    "\n",
    "        chat     = gemini_model.start_chat(history=history)\n",
    "        response = chat.send_message(build_prompt(guidance_input))\n",
    "        message  = response.text.strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        # Fall back to template on API error\n",
    "        templates = FALLBACK_TEMPLATES_KIN if guidance_input.patient_language == \"kinyarwanda\" \\\n",
    "                    else FALLBACK_TEMPLATES\n",
    "        message   = templates.get(priority, templates[\"UNKNOWN\"])\n",
    "        error_msg = str(e)\n",
    "        if verbose:\n",
    "            print(f\"  âŒ API error: {error_msg} â€” using fallback template.\")\n",
    "        output = GuidanceOutput(message=message, priority_used=priority,\n",
    "                                language=guidance_input.patient_language)\n",
    "        return {\"output\": output, \"message\": message, \"safe\": True,\n",
    "                \"violations\": [], \"success\": False, \"error\": error_msg}\n",
    "\n",
    "    # â”€â”€ Safety check â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    is_safe, violations = safety_check(message)\n",
    "\n",
    "    if not is_safe:\n",
    "        # Replace with a safe fallback â€” never expose a flagged message to the patient\n",
    "        templates = FALLBACK_TEMPLATES_KIN if guidance_input.patient_language == \"kinyarwanda\" \\\n",
    "                    else FALLBACK_TEMPLATES\n",
    "        message   = templates.get(priority, templates[\"UNKNOWN\"])\n",
    "        if verbose:\n",
    "            print(f\"  âš ï¸  Safety check FAILED. Violations: {violations}\")\n",
    "            print(\"      Replaced with pre-approved fallback message.\")\n",
    "\n",
    "    output = GuidanceOutput(message=message, priority_used=priority,\n",
    "                            language=guidance_input.patient_language)\n",
    "\n",
    "    if verbose:\n",
    "        _print_message(message, is_safe)\n",
    "\n",
    "    return {\"output\": output, \"message\": message, \"safe\": is_safe,\n",
    "            \"violations\": violations, \"success\": True, \"error\": None}\n",
    "\n",
    "\n",
    "def _print_message(message: str, is_safe: bool = True):\n",
    "    safety_label = \"âœ… SAFE\" if is_safe else \"ðŸ”„ REPLACED (fallback)\"\n",
    "    print(f\"\\n  â”€â”€â”€ PATIENT MESSAGE [{safety_label}] \" + \"â”€\" * 18)\n",
    "    print(f\"\\n  '{message}'\\n\")\n",
    "\n",
    "\n",
    "print(\"âœ… generate_patient_guidance() defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-9",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ”— Section 9 â€” Full Pipeline Integration (Models B + D â†’ E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integration",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_from_pipeline(\n",
    "    model_b_output   : dict,\n",
    "    model_d_output   : dict,\n",
    "    patient_language : str  = \"english\",\n",
    "    location_context : str  = \"\",\n",
    "    verbose          : bool = True,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Compile outputs from Models B and D and generate patient guidance.\n",
    "\n",
    "    Args:\n",
    "        model_b_output   : Dict from Model B's extract_clinical_information().\n",
    "        model_d_output   : Dict from Model D's score_patient_risk().\n",
    "        patient_language : 'english' or 'kinyarwanda'.\n",
    "        location_context : Where the patient should go (e.g. 'Emergency, Desk 3').\n",
    "        verbose          : Print progress.\n",
    "\n",
    "    Returns:\n",
    "        Model E output dict.\n",
    "    \"\"\"\n",
    "    ext   = model_b_output.get(\"extraction_dict\", {})\n",
    "    score = model_d_output.get(\"score_dict\", {})\n",
    "\n",
    "    guidance_input = GuidanceInput(\n",
    "        priority            = score.get(\"priority\", \"MEDIUM\"),\n",
    "        chief_complaint     = ext.get(\"chief_complaint\", \"\"),\n",
    "        duration            = ext.get(\"duration\", \"\"),\n",
    "        severity            = ext.get(\"severity\", \"\"),\n",
    "        associated_symptoms = ext.get(\"associated_symptoms\", []),\n",
    "        red_flags_present   = ext.get(\"red_flags_present\"),\n",
    "        patient_language    = patient_language,\n",
    "        location_context    = location_context,\n",
    "        uncertainty_flag    = score.get(\"confidence\", 1.0) < 0.4,\n",
    "    )\n",
    "\n",
    "    if verbose:\n",
    "        print(\"ðŸ”— Models B + D â†’ Model E\")\n",
    "\n",
    "    return generate_patient_guidance(guidance_input, verbose=verbose)\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# USAGE:\n",
    "#   model_b_result = extract_clinical_information(transcript)\n",
    "#   model_d_result = run_from_model_b(model_b_result)\n",
    "#   model_e_result = run_from_pipeline(model_b_result, model_d_result,\n",
    "#                                      patient_language=\"english\",\n",
    "#                                      location_context=\"Emergency, Desk 3\")\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "print(\"âœ… run_from_pipeline() defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-10",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ§ª Section 10 â€” Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-high",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Test 1: HIGH â€” severe chest pain, red flag, English â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "result_1 = generate_patient_guidance(GuidanceInput(\n",
    "    priority            = \"HIGH\",\n",
    "    chief_complaint     = \"chest pain\",\n",
    "    duration            = \"2 hours\",\n",
    "    severity            = \"severe\",\n",
    "    associated_symptoms = [\"shortness of breath\"],\n",
    "    red_flags_present   = True,\n",
    "    patient_language    = \"english\",\n",
    "    location_context    = \"Emergency, Desk 3\",\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-medium",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Test 2: MEDIUM â€” moderate headache, English â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "result_2 = generate_patient_guidance(GuidanceInput(\n",
    "    priority            = \"MEDIUM\",\n",
    "    chief_complaint     = \"headache\",\n",
    "    duration            = \"2 days\",\n",
    "    severity            = \"moderate\",\n",
    "    associated_symptoms = [\"nausea\"],\n",
    "    red_flags_present   = False,\n",
    "    patient_language    = \"english\",\n",
    "    location_context    = \"Waiting Area B\",\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-low",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Test 3: LOW â€” mild sore throat, English â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "result_3 = generate_patient_guidance(GuidanceInput(\n",
    "    priority         = \"LOW\",\n",
    "    chief_complaint  = \"sore throat\",\n",
    "    duration         = \"3 days\",\n",
    "    severity         = \"mild\",\n",
    "    red_flags_present= False,\n",
    "    patient_language = \"english\",\n",
    "    location_context = \"General Outpatient\",\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-kinyarwanda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Test 4: MEDIUM â€” abdominal pain, Kinyarwanda â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "result_4 = generate_patient_guidance(GuidanceInput(\n",
    "    priority            = \"MEDIUM\",\n",
    "    chief_complaint     = \"ububabare mu nda\",\n",
    "    duration            = \"kuva ejo\",\n",
    "    severity            = \"uburemere\",\n",
    "    associated_symptoms = [\"gushaka kuruka\"],\n",
    "    red_flags_present   = False,\n",
    "    patient_language    = \"kinyarwanda\",\n",
    "    location_context    = \"Waiting Area A\",\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-uncertain",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Test 5: Uncertain â€” sparse data, uncertainty flag triggered â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "result_5 = generate_patient_guidance(GuidanceInput(\n",
    "    priority         = \"MEDIUM\",\n",
    "    chief_complaint  = \"leg hurts\",\n",
    "    patient_language = \"english\",\n",
    "    uncertainty_flag = True,   # sparse data â†’ safe fallback used\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-11",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ“¤ Section 11 â€” Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "def export_guidance(results: list, filepath: str = \"model_e_guidance.json\"):\n",
    "    \"\"\"Save guidance results to JSON and download.\"\"\"\n",
    "    export = {\n",
    "        \"timestamp\" : datetime.datetime.now().isoformat(),\n",
    "        \"model\"     : GEMINI_MODEL_NAME,\n",
    "        \"total\"     : len(results),\n",
    "        \"results\"   : [\n",
    "            {\n",
    "                \"index\"     : i + 1,\n",
    "                \"priority\"  : r[\"output\"].priority_used,\n",
    "                \"language\"  : r[\"output\"].language,\n",
    "                \"message\"   : r[\"message\"],\n",
    "                \"safe\"      : r[\"safe\"],\n",
    "                \"violations\": r[\"violations\"],\n",
    "                \"success\"   : r[\"success\"],\n",
    "            }\n",
    "            for i, r in enumerate(results)\n",
    "        ]\n",
    "    }\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(export, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"\\nâœ… Saved to: {filepath}\")\n",
    "    files.download(filepath)\n",
    "\n",
    "\n",
    "# Export all test results:\n",
    "# export_guidance([result_1, result_2, result_3, result_4, result_5])\n",
    "\n",
    "print(\"âœ… Export function ready. Uncomment the last line to download results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "notes",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ“ Notes\n",
    "\n",
    "| Topic | Detail |\n",
    "|---|---|\n",
    "| **Few-shot examples** | Three primed examples (HIGH, MEDIUM, LOW) are injected as conversation history before the real input. This is the template-LLM hybrid approach â€” Gemini learns style from examples, not from fine-tuning. |\n",
    "| **Safety check** | Every generated message is scanned for forbidden phrases before it reaches the patient. If flagged, the message is silently replaced with a pre-approved fallback. |\n",
    "| **Uncertainty flag** | If Model D's confidence is below 0.4, the uncertainty flag is set and a neutral holding message is used instead of generating a specific one. |\n",
    "| **Fallback templates** | Available in both English and Kinyarwanda. They are static, audited, and always safe â€” used on API errors, safety failures, or uncertain inputs. |\n",
    "| **Temperature = 0.3** | Slightly warmer than other models â€” allows natural, human-sounding phrasing while still staying within guardrails. |\n",
    "| **No re-inference** | Model E reads `guidance_input` and explains it. It does not re-evaluate symptoms, reassess priority, or reason about the patient's condition. |\n",
    "| **Location context** | Pass the physical location (e.g. `\"Emergency, Desk 3\"`) so the message tells the patient exactly where to go. |"
   ]
  }
 ]
}
