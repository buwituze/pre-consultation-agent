{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/buwituze/pre-consultation-agent/blob/main/speech_to_text_asr_a.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "title-cell",
      "metadata": {
        "id": "title-cell"
      },
      "source": [
        "# üéôÔ∏è Speech-to-Text Module ‚Äî Kinyarwanda & English ASR\n",
        "\n",
        "**Purpose:** Convert patient speech (Kinyarwanda or English) into accurate transcribed text with confidence scores.\n",
        "\n",
        "| | |\n",
        "|---|---|\n",
        "| **Input** | Raw audio (`.wav`, `.mp3`, `.flac`, `.m4a`) |\n",
        "| **Output** | Transcribed text + confidence score per segment |\n",
        "| **Kinyarwanda Model** | `akera/whisper-large-v3-kin-200h-v2` |\n",
        "| **English Model** | `openai/whisper-large-v3` |\n",
        "\n",
        "### Pipeline Overview\n",
        "```\n",
        "Audio Input\n",
        "    ‚îÇ\n",
        "    ‚ñº\n",
        "Language Detection  ‚îÄ‚îÄ‚ñ∫  Kinyarwanda  ‚îÄ‚îÄ‚ñ∫  Akera Whisper Model\n",
        "    ‚îÇ                                            ‚îÇ\n",
        "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫  English       ‚îÄ‚îÄ‚ñ∫  OpenAI Whisper Model\n",
        "                                                 ‚îÇ\n",
        "                                    Transcription + Confidence Score\n",
        "```\n",
        "\n",
        "> ‚ö†Ô∏è **Runtime:** Set to **GPU (T4 or better)** via `Runtime > Change runtime type` for best performance."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-1",
      "metadata": {
        "id": "section-1"
      },
      "source": [
        "---\n",
        "## üì¶ Section 1 ‚Äî Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "install-deps",
      "metadata": {
        "id": "install-deps"
      },
      "outputs": [],
      "source": [
        "# Install all required libraries\n",
        "!pip install -q -U transformers accelerate\n",
        "!pip install -q langdetect librosa soundfile torchaudio\n",
        "!pip install -q datasets  # optional: for loading test audio\n",
        "\n",
        "print(\"‚úÖ Dependencies installed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-2",
      "metadata": {
        "id": "section-2"
      },
      "source": [
        "---\n",
        "## üîß Section 2 ‚Äî Imports & Device Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "imports",
      "metadata": {
        "id": "imports"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq, pipeline\n",
        "from langdetect import detect, LangDetectException\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# ‚îÄ‚îÄ Device configuration ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "TORCH_DTYPE = torch.float16 if DEVICE == \"cuda\" else torch.float32\n",
        "\n",
        "print(f\"üñ•Ô∏è  Device  : {DEVICE.upper()}\")\n",
        "if DEVICE == \"cuda\":\n",
        "    print(f\"üéÆ  GPU     : {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"üíæ  VRAM    : {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No GPU detected ‚Äî inference will be slow on CPU.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-3",
      "metadata": {
        "id": "section-3"
      },
      "source": [
        "---\n",
        "## ü§ñ Section 3 ‚Äî Load ASR Models\n",
        "\n",
        "Both models are loaded once and cached. Loading takes ~2‚Äì5 minutes on first run (downloading weights)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "load-models",
      "metadata": {
        "id": "load-models"
      },
      "outputs": [],
      "source": [
        "# ‚îÄ‚îÄ Model identifiers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "KIN_MODEL_ID = \"akera/whisper-large-v3-kin-200h-v2\"   # Kinyarwanda\n",
        "ENG_MODEL_ID = \"openai/whisper-large-v3\"               # English\n",
        "\n",
        "TARGET_SAMPLE_RATE = 16_000  # Whisper expects 16 kHz audio\n",
        "\n",
        "def load_whisper_pipeline(model_id: str, language: Optional[str] = None) -> pipeline:\n",
        "    \"\"\"Load a Whisper model as a HuggingFace ASR pipeline.\"\"\"\n",
        "    print(f\"  Loading: {model_id} ...\")\n",
        "    pipe = pipeline(\n",
        "        task=\"automatic-speech-recognition\",\n",
        "        model=model_id,\n",
        "        torch_dtype=TORCH_DTYPE,\n",
        "        device=DEVICE,\n",
        "        # Return per-token log probabilities for confidence scoring\n",
        "        return_timestamps=True,\n",
        "    )\n",
        "    # Pin forced language decoder token if specified\n",
        "    if language:\n",
        "        pipe.model.config.forced_decoder_ids = (\n",
        "            pipe.tokenizer.get_decoder_prompt_ids(language=language, task=\"transcribe\")\n",
        "        )\n",
        "    print(f\"  ‚úÖ Loaded: {model_id}\")\n",
        "    return pipe\n",
        "\n",
        "\n",
        "print(\"\\nüîÑ Loading Kinyarwanda model...\")\n",
        "kin_pipe = load_whisper_pipeline(KIN_MODEL_ID)\n",
        "\n",
        "print(\"\\nüîÑ Loading English model...\")\n",
        "eng_pipe = load_whisper_pipeline(ENG_MODEL_ID, language=\"english\")\n",
        "\n",
        "print(\"\\nüéâ Both models ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-4",
      "metadata": {
        "id": "section-4"
      },
      "source": [
        "---\n",
        "## üåê Section 4 ‚Äî Language Detection\n",
        "\n",
        "A lightweight text-based detection step identifies the dominant language of each audio segment **after** a quick Whisper decode pass. This avoids a separate audio-level classifier and leverages Whisper's multilingual capability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lang-detection",
      "metadata": {
        "id": "lang-detection"
      },
      "outputs": [],
      "source": [
        "# ‚îÄ‚îÄ Supported language codes ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# langdetect uses ISO 639-1 codes; Kinyarwanda = 'rw'\n",
        "KINYARWANDA_CODES = {\"rw\"}        # ISO codes that map to Kinyarwanda\n",
        "ENGLISH_CODES     = {\"en\"}        # ISO codes that map to English\n",
        "\n",
        "\n",
        "def detect_language_from_text(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Detect dominant language from transcribed text.\n",
        "\n",
        "    Returns:\n",
        "        'kinyarwanda' | 'english' | 'unknown'\n",
        "    \"\"\"\n",
        "    if not text or len(text.strip()) < 5:\n",
        "        return \"unknown\"\n",
        "    try:\n",
        "        lang_code = detect(text)\n",
        "        if lang_code in KINYARWANDA_CODES:\n",
        "            return \"kinyarwanda\"\n",
        "        elif lang_code in ENGLISH_CODES:\n",
        "            return \"english\"\n",
        "        else:\n",
        "            return \"unknown\"\n",
        "    except LangDetectException:\n",
        "        return \"unknown\"\n",
        "\n",
        "\n",
        "def detect_language_from_audio(audio_array: np.ndarray, sample_rate: int) -> str:\n",
        "    \"\"\"\n",
        "    Detect language by doing a quick decode with the English Whisper model\n",
        "    and reading its internal language prediction token.\n",
        "\n",
        "    Falls back to text-based detection if token is unavailable.\n",
        "    \"\"\"\n",
        "    # Use the English (multilingual) model for language ID\n",
        "    result = eng_pipe(\n",
        "        {\"array\": audio_array, \"sampling_rate\": sample_rate},\n",
        "        generate_kwargs={\"task\": \"transcribe\", \"language\": None},  # let model decide\n",
        "        return_timestamps=False,\n",
        "    )\n",
        "    detected_text = result.get(\"text\", \"\")\n",
        "    return detect_language_from_text(detected_text)\n",
        "\n",
        "\n",
        "print(\"‚úÖ Language detection functions defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-5",
      "metadata": {
        "id": "section-5"
      },
      "source": [
        "---\n",
        "## üìä Section 5 ‚Äî Confidence Scoring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "confidence",
      "metadata": {
        "id": "confidence"
      },
      "outputs": [],
      "source": [
        "def compute_confidence_from_chunks(chunks: list) -> float:\n",
        "    \"\"\"\n",
        "    Derive an aggregate confidence score (0.0 ‚Äì 1.0) from Whisper\n",
        "    timestamp chunks.\n",
        "\n",
        "    Whisper doesn't expose token-level log-probs through the pipeline directly,\n",
        "    so we approximate confidence using segment-level heuristics:\n",
        "      - Presence and count of clean timestamp chunks\n",
        "      - Absence of [BLANK_AUDIO] / repetition artifacts\n",
        "\n",
        "    For production, replace with log-prob extraction from model.generate().\n",
        "    \"\"\"\n",
        "    if not chunks:\n",
        "        return 0.0\n",
        "\n",
        "    penalties = 0.0\n",
        "    for chunk in chunks:\n",
        "        text = chunk.get(\"text\", \"\")\n",
        "        # Common low-quality Whisper outputs\n",
        "        if any(tag in text for tag in [\"[BLANK_AUDIO]\", \"[MUSIC]\", \"‚ô™\"]):\n",
        "            penalties += 0.3\n",
        "        # Detect repetition (hallucination signal)\n",
        "        words = text.strip().split()\n",
        "        if len(words) > 2 and len(set(words)) / len(words) < 0.5:\n",
        "            penalties += 0.2\n",
        "\n",
        "    raw_score = max(0.0, 1.0 - (penalties / max(len(chunks), 1)))\n",
        "    return round(raw_score, 3)\n",
        "\n",
        "\n",
        "print(\"‚úÖ Confidence scoring function defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-6",
      "metadata": {
        "id": "section-6"
      },
      "source": [
        "---\n",
        "## üîä Section 6 ‚Äî Audio Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "audio-preprocessing",
      "metadata": {
        "id": "audio-preprocessing"
      },
      "outputs": [],
      "source": [
        "def load_and_preprocess_audio(audio_path: str, target_sr: int = TARGET_SAMPLE_RATE) -> tuple:\n",
        "    \"\"\"\n",
        "    Load an audio file, resample to target sample rate, and convert to mono.\n",
        "\n",
        "    Args:\n",
        "        audio_path : Path to audio file (.wav / .mp3 / .flac / .m4a)\n",
        "        target_sr  : Target sample rate (default 16 kHz for Whisper)\n",
        "\n",
        "    Returns:\n",
        "        (audio_array: np.ndarray, sample_rate: int)\n",
        "    \"\"\"\n",
        "    audio_array, sr = librosa.load(audio_path, sr=target_sr, mono=True)\n",
        "    # Normalize amplitude to [-1, 1]\n",
        "    if audio_array.max() > 0:\n",
        "        audio_array = audio_array / np.abs(audio_array).max()\n",
        "    print(f\"  üìÅ File    : {audio_path}\")\n",
        "    print(f\"  ‚è±Ô∏è  Duration: {len(audio_array)/sr:.2f}s  |  Sample rate: {sr} Hz\")\n",
        "    return audio_array, sr\n",
        "\n",
        "\n",
        "def split_audio_into_segments(audio_array: np.ndarray,\n",
        "                               sample_rate: int,\n",
        "                               segment_duration_s: float = 30.0) -> list:\n",
        "    \"\"\"\n",
        "    Split audio into fixed-length segments for per-segment language detection.\n",
        "    Whisper performs best on ‚â§30s chunks.\n",
        "\n",
        "    Returns:\n",
        "        List of (start_time, end_time, audio_chunk) tuples.\n",
        "    \"\"\"\n",
        "    segment_len = int(segment_duration_s * sample_rate)\n",
        "    segments = []\n",
        "    for i, start in enumerate(range(0, len(audio_array), segment_len)):\n",
        "        chunk = audio_array[start : start + segment_len]\n",
        "        start_t = start / sample_rate\n",
        "        end_t   = min((start + segment_len) / sample_rate,\n",
        "                      len(audio_array) / sample_rate)\n",
        "        segments.append((start_t, end_t, chunk))\n",
        "    print(f\"  üî™ Split into {len(segments)} segment(s) of ‚â§{segment_duration_s}s each.\")\n",
        "    return segments\n",
        "\n",
        "\n",
        "print(\"‚úÖ Audio preprocessing functions defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-7",
      "metadata": {
        "id": "section-7"
      },
      "source": [
        "---\n",
        "## üöÄ Section 7 ‚Äî Core Transcription Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "transcription-pipeline",
      "metadata": {
        "id": "transcription-pipeline"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class SegmentResult:\n",
        "    \"\"\"Holds the result for a single audio segment.\"\"\"\n",
        "    start_time  : float\n",
        "    end_time    : float\n",
        "    language    : str\n",
        "    text        : str\n",
        "    confidence  : float\n",
        "    model_used  : str\n",
        "\n",
        "\n",
        "def transcribe_segment(\n",
        "    audio_chunk  : np.ndarray,\n",
        "    sample_rate  : int,\n",
        "    forced_lang  : Optional[str] = None,\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    Transcribe a single audio segment using the appropriate model.\n",
        "\n",
        "    Args:\n",
        "        audio_chunk : 1-D numpy array of audio samples\n",
        "        sample_rate : Sample rate of the audio\n",
        "        forced_lang : 'kinyarwanda' | 'english' | None (auto-detect)\n",
        "\n",
        "    Returns:\n",
        "        dict with keys: text, language, confidence, model_used, chunks\n",
        "    \"\"\"\n",
        "    audio_input = {\"array\": audio_chunk, \"sampling_rate\": sample_rate}\n",
        "\n",
        "    # ‚îÄ‚îÄ Step 1: Language detection ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    if forced_lang:\n",
        "        detected_lang = forced_lang\n",
        "    else:\n",
        "        detected_lang = detect_language_from_audio(audio_chunk, sample_rate)\n",
        "        if detected_lang == \"unknown\":\n",
        "            # Default to Kinyarwanda for this medical domain (primary patient language)\n",
        "            detected_lang = \"kinyarwanda\"\n",
        "            print(f\"  ‚ö†Ô∏è  Language unclear ‚Üí defaulting to Kinyarwanda\")\n",
        "\n",
        "    # ‚îÄ‚îÄ Step 2: Route to appropriate model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    if detected_lang == \"kinyarwanda\":\n",
        "        active_pipe = kin_pipe\n",
        "        model_name  = KIN_MODEL_ID\n",
        "        gen_kwargs  = {\"task\": \"transcribe\"}\n",
        "    else:  # english\n",
        "        active_pipe = eng_pipe\n",
        "        model_name  = ENG_MODEL_ID\n",
        "        gen_kwargs  = {\"task\": \"transcribe\", \"language\": \"english\"}\n",
        "\n",
        "    # ‚îÄ‚îÄ Step 3: Transcribe ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "    result = active_pipe(\n",
        "        audio_input,\n",
        "        generate_kwargs=gen_kwargs,\n",
        "        return_timestamps=True,\n",
        "    )\n",
        "\n",
        "    text   = result.get(\"text\", \"\").strip()\n",
        "    chunks = result.get(\"chunks\", [])\n",
        "    conf   = compute_confidence_from_chunks(chunks)\n",
        "\n",
        "    return {\n",
        "        \"text\"       : text,\n",
        "        \"language\"   : detected_lang,\n",
        "        \"confidence\" : conf,\n",
        "        \"model_used\" : model_name,\n",
        "        \"chunks\"     : chunks,\n",
        "    }\n",
        "\n",
        "\n",
        "def transcribe_audio_file(\n",
        "    audio_path       : str,\n",
        "    forced_lang      : Optional[str] = None,\n",
        "    segment_duration : float = 30.0,\n",
        "    verbose          : bool = True,\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    Full pipeline: load ‚Üí segment ‚Üí detect language ‚Üí transcribe ‚Üí aggregate.\n",
        "\n",
        "    Args:\n",
        "        audio_path       : Path to the audio file.\n",
        "        forced_lang      : Override language detection ('kinyarwanda' or 'english').\n",
        "        segment_duration : Duration of each segment in seconds (max 30 for Whisper).\n",
        "        verbose          : Print progress logs.\n",
        "\n",
        "    Returns:\n",
        "        {\n",
        "          'full_text'         : str,          # concatenated transcript\n",
        "          'mean_confidence'   : float,        # average confidence score\n",
        "          'dominant_language' : str,\n",
        "          'segments'          : [SegmentResult, ...]\n",
        "        }\n",
        "    \"\"\"\n",
        "    if verbose:\n",
        "        print(\"\\n\" + \"‚ïê\" * 60)\n",
        "        print(\" üéôÔ∏è  TRANSCRIPTION PIPELINE\")\n",
        "        print(\"‚ïê\" * 60)\n",
        "\n",
        "    # 1. Load & preprocess\n",
        "    audio_array, sr = load_and_preprocess_audio(audio_path)\n",
        "\n",
        "    # 2. Split into segments\n",
        "    segments_raw = split_audio_into_segments(audio_array, sr, segment_duration)\n",
        "\n",
        "    # 3. Transcribe each segment\n",
        "    segment_results = []\n",
        "    for idx, (start_t, end_t, chunk) in enumerate(segments_raw):\n",
        "        if verbose:\n",
        "            print(f\"\\n  ‚ñ∂ Segment {idx+1}/{len(segments_raw)} \"\n",
        "                  f\"[{start_t:.1f}s ‚Üí {end_t:.1f}s]\")\n",
        "\n",
        "        seg_out = transcribe_segment(chunk, sr, forced_lang=forced_lang)\n",
        "\n",
        "        sr_obj = SegmentResult(\n",
        "            start_time = start_t,\n",
        "            end_time   = end_t,\n",
        "            language   = seg_out[\"language\"],\n",
        "            text       = seg_out[\"text\"],\n",
        "            confidence = seg_out[\"confidence\"],\n",
        "            model_used = seg_out[\"model_used\"],\n",
        "        )\n",
        "        segment_results.append(sr_obj)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"     Language   : {sr_obj.language}\")\n",
        "            print(f\"     Model      : {sr_obj.model_used.split('/')[-1]}\")\n",
        "            print(f\"     Confidence : {sr_obj.confidence:.3f}\")\n",
        "            print(f\"     Text       : {sr_obj.text[:120]}{'...' if len(sr_obj.text)>120 else ''}\")\n",
        "\n",
        "    # 4. Aggregate results\n",
        "    full_text  = \" \".join(s.text for s in segment_results if s.text)\n",
        "    mean_conf  = np.mean([s.confidence for s in segment_results]) if segment_results else 0.0\n",
        "    lang_votes = [s.language for s in segment_results]\n",
        "    dominant   = max(set(lang_votes), key=lang_votes.count) if lang_votes else \"unknown\"\n",
        "\n",
        "    if verbose:\n",
        "        print(\"\\n\" + \"‚ïê\" * 60)\n",
        "        print(\" ‚úÖ  TRANSCRIPTION COMPLETE\")\n",
        "        print(\"‚ïê\" * 60)\n",
        "        print(f\"  Dominant Language : {dominant}\")\n",
        "        print(f\"  Mean Confidence   : {mean_conf:.3f}\")\n",
        "        print(f\"  Full Transcript   :\\n\\n  {full_text}\\n\")\n",
        "\n",
        "    return {\n",
        "        \"full_text\"         : full_text,\n",
        "        \"mean_confidence\"   : round(float(mean_conf), 3),\n",
        "        \"dominant_language\" : dominant,\n",
        "        \"segments\"          : segment_results,\n",
        "    }\n",
        "\n",
        "\n",
        "print(\"‚úÖ Transcription pipeline defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-8",
      "metadata": {
        "id": "section-8"
      },
      "source": [
        "---\n",
        "## üß™ Section 8 ‚Äî Run Inference\n",
        "\n",
        "### Option A ‚Äî Transcribe an uploaded file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "option-a",
      "metadata": {
        "id": "option-a"
      },
      "outputs": [],
      "source": [
        "# ‚îÄ‚îÄ Option A: Upload your own audio file ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "from google.colab import files\n",
        "\n",
        "print(\"üìÇ Please upload an audio file (.wav / .mp3 / .flac / .m4a):\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "if uploaded:\n",
        "    audio_filename = list(uploaded.keys())[0]\n",
        "    result = transcribe_audio_file(\n",
        "        audio_path   = audio_filename,\n",
        "        forced_lang  = None,   # None = auto-detect | 'kinyarwanda' | 'english'\n",
        "        segment_duration = 30.0,\n",
        "        verbose      = True,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "option-b-header",
      "metadata": {
        "id": "option-b-header"
      },
      "source": [
        "### Option B ‚Äî Transcribe a local path (e.g. Google Drive)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "option-b",
      "metadata": {
        "id": "option-b"
      },
      "outputs": [],
      "source": [
        "# ‚îÄ‚îÄ Option B: Mount Google Drive and provide a path ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "#\n",
        "# audio_path = \"/content/drive/MyDrive/patient_audio/sample.wav\"\n",
        "# result = transcribe_audio_file(audio_path, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "option-c-header",
      "metadata": {
        "id": "option-c-header"
      },
      "source": [
        "### Option C ‚Äî Test with a sample audio from HuggingFace datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "option-c",
      "metadata": {
        "id": "option-c"
      },
      "outputs": [],
      "source": [
        "# ‚îÄ‚îÄ Option C: Quick smoke-test using a public dataset sample ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "from datasets import load_dataset\n",
        "\n",
        "print(\"üîÑ Loading a sample from the LibriSpeech test-clean dataset...\")\n",
        "ds = load_dataset(\"hf-internal-testing/librispeech_asr_demo\",\n",
        "                  \"clean\", split=\"validation\", trust_remote_code=True)\n",
        "\n",
        "sample     = ds[0]\n",
        "audio_arr  = np.array(sample[\"audio\"][\"array\"], dtype=np.float32)\n",
        "sample_sr  = sample[\"audio\"][\"sampling_rate\"]\n",
        "\n",
        "print(f\"  Reference text: {sample['text']}\\n\")\n",
        "\n",
        "# Run transcription directly on the numpy array (skip file load)\n",
        "seg_out = transcribe_segment(audio_arr, sample_sr, forced_lang=\"english\")\n",
        "\n",
        "print(\"\\nüìù Transcription Results:\")\n",
        "print(f\"  Language   : {seg_out['language']}\")\n",
        "print(f\"  Model      : {seg_out['model_used'].split('/')[-1]}\")\n",
        "print(f\"  Confidence : {seg_out['confidence']:.3f}\")\n",
        "print(f\"  Text       : {seg_out['text']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-9",
      "metadata": {
        "id": "section-9"
      },
      "source": [
        "---\n",
        "## üìã Section 9 ‚Äî Inspect Results & Export"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "inspect-results",
      "metadata": {
        "id": "inspect-results"
      },
      "outputs": [],
      "source": [
        "# ‚îÄ‚îÄ View segment-by-segment breakdown ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# Run this cell after Option A or B above\n",
        "\n",
        "try:\n",
        "    print(f\"\\n{'‚îÄ'*60}\")\n",
        "    print(f\" SEGMENT BREAKDOWN ({len(result['segments'])} segment(s))\")\n",
        "    print(f\"{'‚îÄ'*60}\")\n",
        "    for i, seg in enumerate(result[\"segments\"]):\n",
        "        print(f\"\\n  [{i+1}] {seg.start_time:.1f}s ‚Üí {seg.end_time:.1f}s\")\n",
        "        print(f\"       Language   : {seg.language}\")\n",
        "        print(f\"       Confidence : {seg.confidence:.3f}\")\n",
        "        print(f\"       Model      : {seg.model_used.split('/')[-1]}\")\n",
        "        print(f\"       Text       : {seg.text}\")\n",
        "\n",
        "    print(f\"\\n{'‚îÄ'*60}\")\n",
        "    print(f\" FULL TRANSCRIPT\")\n",
        "    print(f\"{'‚îÄ'*60}\")\n",
        "    print(result[\"full_text\"])\n",
        "    print(f\"\\n  Mean Confidence : {result['mean_confidence']:.3f}\")\n",
        "    print(f\"  Dominant Language: {result['dominant_language']}\")\n",
        "\n",
        "except NameError:\n",
        "    print(\"‚ö†Ô∏è  No result found ‚Äî please run Section 8 first.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "export-results",
      "metadata": {
        "id": "export-results"
      },
      "outputs": [],
      "source": [
        "# ‚îÄ‚îÄ Export transcript to a JSON file ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "import json, datetime\n",
        "\n",
        "try:\n",
        "    export_data = {\n",
        "        \"timestamp\"         : datetime.datetime.now().isoformat(),\n",
        "        \"dominant_language\" : result[\"dominant_language\"],\n",
        "        \"mean_confidence\"   : result[\"mean_confidence\"],\n",
        "        \"full_text\"         : result[\"full_text\"],\n",
        "        \"segments\": [\n",
        "            {\n",
        "                \"segment\"    : i + 1,\n",
        "                \"start_time\" : seg.start_time,\n",
        "                \"end_time\"   : seg.end_time,\n",
        "                \"language\"   : seg.language,\n",
        "                \"confidence\" : seg.confidence,\n",
        "                \"model_used\" : seg.model_used,\n",
        "                \"text\"       : seg.text,\n",
        "            }\n",
        "            for i, seg in enumerate(result[\"segments\"])\n",
        "        ],\n",
        "    }\n",
        "\n",
        "    output_path = \"transcript_output.json\"\n",
        "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(export_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"‚úÖ Transcript saved to: {output_path}\")\n",
        "\n",
        "    # Download to local machine\n",
        "    from google.colab import files\n",
        "    files.download(output_path)\n",
        "\n",
        "except NameError:\n",
        "    print(\"‚ö†Ô∏è  No result found ‚Äî please run Section 8 first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "section-10",
      "metadata": {
        "id": "section-10"
      },
      "source": [
        "---\n",
        "## üîÅ Section 10 ‚Äî Batch Transcription (Multiple Files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "batch-transcription",
      "metadata": {
        "id": "batch-transcription"
      },
      "outputs": [],
      "source": [
        "def batch_transcribe(audio_paths: list, forced_lang: Optional[str] = None) -> list:\n",
        "    \"\"\"\n",
        "    Transcribe a list of audio files and return all results.\n",
        "\n",
        "    Args:\n",
        "        audio_paths : List of file paths to audio files.\n",
        "        forced_lang : Optional language override for all files.\n",
        "\n",
        "    Returns:\n",
        "        List of result dicts (same structure as transcribe_audio_file).\n",
        "    \"\"\"\n",
        "    all_results = []\n",
        "    for idx, path in enumerate(audio_paths):\n",
        "        print(f\"\\nüìÑ File {idx+1}/{len(audio_paths)}: {path}\")\n",
        "        try:\n",
        "            res = transcribe_audio_file(path, forced_lang=forced_lang, verbose=True)\n",
        "            res[\"file\"] = path\n",
        "            all_results.append(res)\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ùå Error: {e}\")\n",
        "            all_results.append({\"file\": path, \"error\": str(e)})\n",
        "    return all_results\n",
        "\n",
        "\n",
        "# Example usage ‚Äî uncomment and adjust paths:\n",
        "# batch_results = batch_transcribe(\n",
        "#     audio_paths = [\"patient_01.wav\", \"patient_02.wav\", \"patient_03.mp3\"],\n",
        "#     forced_lang = None,  # auto-detect\n",
        "# )\n",
        "\n",
        "print(\"‚úÖ Batch transcription function defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "notes-cell",
      "metadata": {
        "id": "notes-cell"
      },
      "source": [
        "---\n",
        "## üìù Notes & Known Limitations\n",
        "\n",
        "| Topic | Detail |\n",
        "|---|---|\n",
        "| **Confidence scores** | Approximated from segment heuristics. For token-level log-probs, use `model.generate()` with `return_dict_in_generate=True`. |\n",
        "| **Code-switching** | Audio with heavy Kinyarwanda‚ÄìEnglish mixing may be routed to a single model. Consider splitting at silence boundaries. |\n",
        "| **Background noise** | Whisper large-v3 is robust to light noise. Heavy noise may require a denoising pre-step (e.g., `noisereduce` library). |\n",
        "| **Memory** | Loading both models simultaneously requires ~10‚Äì14 GB VRAM. If OOM errors occur, load models one at a time. |\n",
        "| **Medical vocabulary** | No domain-adapted vocabulary. Downstream correction of medical terms is recommended before use. |\n",
        "| **Real-time use** | This notebook is batch-oriented. For streaming, integrate with `pyaudio` + chunked inference. |"
      ]
    }
  ]
}